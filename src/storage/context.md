# Storage Context

## Purpose
PostgreSQL database layer with pgvector extension for the RAG (Retrieval-Augmented Generation) system. Handles connection pooling, schema initialization, article/chunk storage, vector similarity search, HITL report management, and **narrative storyline persistence** (storylines, graph edges, article-storyline links).

## Architecture Role
Central persistence layer between the processing pipeline and intelligence generation. All NLP-processed articles flow here for storage, and the LLM modules retrieve context via semantic search. Also stores generated reports, human feedback, **narrative storylines with momentum scoring**, and **inter-storyline graph edges**.

## Key Files

- `database.py` - `DatabaseManager` class (1900+ lines)

  **Connection Management:**
  - `SimpleConnectionPool` (min=1, max=10 connections)
  - `get_connection()` context manager with auto-commit/rollback
  - `register_vector()` for pgvector type handling

  **Schema Initialization (`init_db()`):**
  - `articles` table - Full articles with NLP metadata, embeddings (384-dim)
  - `chunks` table - RAG chunks with embeddings for semantic search
  - `reports` table - LLM-generated intelligence reports
  - `report_feedback` table - Human corrections and ratings
  - `entities` table - Named entities with geocoding (for Intelligence Map)
  - **`storylines` table** - Narrative threads: title, summary, embedding, momentum_score, narrative_status
  - **`storyline_edges` table** - Graph edges: source/target storyline, Jaccard weight, relation_type
  - **`article_storylines` table** - Junction: article_id ↔ storyline_id, relevance_score
  - HNSW indexes for fast approximate nearest neighbor search

  **Core Operations:**
  - `save_article()` / `batch_save()` - Store articles with content-hash deduplication
  - `semantic_search()` - Vector similarity search on chunks with filters
  - `full_text_search()` - PostgreSQL `ts_query` for keyword search
  - `hybrid_search()` - Combines vector + keyword with RRF fusion
  - `save_report()` / `update_report()` - Report lifecycle management
  - `save_feedback()` / `get_report_feedback()` - HITL feedback storage

  **Specialized Methods:**
  - `get_all_article_embeddings(days, exclude_assigned)` - Returns articles with embeddings for storyline clustering; `exclude_assigned=True` skips articles already in `article_storylines` (used by `NarrativeProcessor.process_daily_batch`)
  - `get_entities_with_coordinates()` - GeoJSON output for map
  - `update_report_embedding(report_id, embedding)` - Updates `summary_vector` for a report
  - `semantic_search_reports()` - Search reports by embedding (for Oracle)
  - `get_reports_by_date_range()` - For weekly meta-analysis

## Database Views (Narrative Engine)

| View | Purpose |
|------|---------|
| `v_active_storylines` | Active storylines ordered by momentum_score DESC, with article_count |
| `v_storyline_graph` | Edges between active storylines, includes source/target titles |

These views are consumed by both the report generator (top 10 storylines for narrative context) and the API (`/api/v1/stories/graph`).

**Important:** `DatabaseManager` does NOT have `get_active_storylines()` or `get_storyline_graph()` methods. The API router (`src/api/routers/stories.py`) queries these views directly via raw SQL. The NarrativeProcessor also reads/writes storyline data via raw SQL within its own stage methods, not through DatabaseManager helper methods.

## Dependencies

- **Internal**: `src/utils/logger`
- **External**:
  - `psycopg2` - PostgreSQL adapter
  - `psycopg2.pool.SimpleConnectionPool` - Connection pooling
  - `pgvector.psycopg2` - Vector type registration
  - `psycopg2.extras.Json` - JSONB handling

## Data Flow

- **Input**:
  - Processed articles with NLP data and embeddings from `src/nlp/`
  - Query embeddings for semantic search from `src/llm/`
  - Generated reports from `src/llm/report_generator.py`
  - Human feedback from `src/hitl/`
  - **Storyline data from `src/nlp/narrative_processor.py`** (storylines, edges, article links)

- **Output**:
  - Retrieved chunks for RAG context
  - Article metadata and statistics
  - Reports for review/editing
  - GeoJSON entities for Intelligence Map
  - **Storyline graph data for API** (nodes, edges, detail)
  - **Narrative context for report generation** (top storylines by momentum)
  - Feedback data for prompt improvement

## Key Tables

| Table | Purpose |
|-------|---------|
| `articles` | Full articles, embeddings, NLP metadata, entities (JSONB) |
| `chunks` | 500-word chunks with 384-dim embeddings for RAG |
| `reports` | Generated intelligence reports (draft/final/status) |
| `report_feedback` | Human corrections, ratings, comments |
| `entities` | Named entities with coordinates for map |
| **`storylines`** | Narrative threads: title, summary, embedding, momentum_score, narrative_status (emerging/active/stabilized/archived) |
| **`storyline_edges`** | Graph edges: source_story_id → target_story_id, Jaccard weight, relation_type |
| **`article_storylines`** | Junction table: article_id ↔ storyline_id, relevance_score |
| `market_data` | OHLCV time series from Yahoo Finance |
| `ticker_mappings` | Entity → Stock ticker mappings |
